raw {
  check-syntax-analyzers = false
}
raw {
 compiler {
    inferrer {
        # Number of inferrer threads.
        thread-pool-size = 8
        # Timeout for an inference request.
        timeout = 30s
        # Number of entries in the inferrer cache.
        cache-size = 100
        # Time after which an inferrer cache entry expires if it is not accessed!
        expiry = 30s
    }
    output-format = "hjson"
    windows-line-ending = false
    skip-phases = []
    jvm {
      compilation-timeout = 300 s
    }
    scala2 {
      # Settings used to compile queries, same format as if passed to scalac in the command line
      # https://docs.scala-lang.org/overviews/compiler-options/index.html#introduction
      # See scalac -opt:help for possible values.

      # (msb): I think these make more sense, but keeping the "old ones" for now.
      #settings = "-opt:inline:raw.runtime.**,inline:raw.query.**"

      settings = "-opt:unreachable-code,box-unbox,simplify-jumps,redundant-casts,copy-propagation"

      classpath = []
      compilation-directory = ${java.io.tmpdir}"/raw-compilation"
      pretty-print-code = true
      # Maximum number of classes to load on startup. If more than this limit is reached, delete code cache.
      # Use 0 to always delete code cache and start fresh.
      # Use -1 for no limit.
      max-classes-on-startup = 0
    }
  }
}
raw.rest.client {
  async-request-retries = 10

  service-not-available-retries = 10
  service-not-available-retry-interval = 1s

  connect-timeout = 20s
  socket-timeout = 120s

  max-conn-per-route = 20
  max-conn-total = 100
}
raw.inferrer.local {

    # Number of files to sample while inferring wildcards or folders. Use -1 for all.
    sample-files = 10

    encoding-detection-read-size = 1M

    csv {
      sample-size = 10000
      # How many lines to use to guess the separator
      separator-sample-size = 100
      # Weight for quoted values when choosing delimiter in csv files
      # 0.3 is a magic number, it was chosen so test "delimiter comas in quotes" works
      quoted-weight = 3.0
    }

    hjson {
      sample-size = 10000
    }

    json {
      sample-size = 10000
    }

    text {
      sample-size = 10000
    }

    xml {
      sample-size = 10000
    }

    # Buffered seekable input stream can be used for text-based formats.
    use-buffered-seekable-is = true

}
raw.sources {
  rdbms {
    connect-timeout = 30 s
    read-timeout = 300 s
    network-timeout = 300 s
    login-timeout = 30 s
  }
}
raw.sources.dropbox.clientId = ""
raw.sources.bytestream.http {
  connect-timeout = 20 s
  read-timeout = 120 s
  conn-pool-max-per-route = 4096
  conn-pool-max-total = 32768
}
raw.sources.s3 {
  connect-timeout = 60 s
  read-timeout = 120 s
  max-connections = 50

  # Hadoop s3a filesystem will make a nested loop of retries with the 2 next settings.
  # so if max-retries = 7 and max-attempts = 3, it will make 7*3 = 21 retries
  # see fs.s3a.attempts.maximum and fs.s3a.retry.limit
  max-retries = 10
  max-attempts = 0
  # Initial delay between s3a retries, see fs.s3a.retry.interval
  retry-interval = 100 ms
  tmp-dir = ${java.io.tmpdir}/s3

  default-region = eu-west-1
}